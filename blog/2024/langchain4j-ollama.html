<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
    <title>Running a local LLM on Ollama and LangChain4J</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">
    <meta name="keywords" content="">
    <meta name="generator" content="JBake">

    <!-- Le styles -->
    <!--link href="../../css/bootstrap.min.css" rel="stylesheet"-->
    <link href="../../css/themes/sandstone.css" rel="stylesheet">
    <link href="../../css/asciidoctor.css" rel="stylesheet">
    <link href="../../css/base.css" rel="stylesheet">
    <link href="../../css/prettify.css" rel="stylesheet">

    <!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-114748485-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-114748485-1');
    </script>
    
  </head>
  <body onload="prettyPrint()">
    <div id="wrap">
	
	<!-- Fixed navbar -->  

<nav class="navbar navbar-expand-lg fixed-top navbar-dark bg-primary">

  <div class="container">
    <a class="navbar-brand" href="../../">dplatz.de</a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarColor01" aria-controls="navbarColor01" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbarColor01">
      <ul class="navbar-nav mr-auto">
        <li class="nav-item"><a class="nav-link" href="../../index.html">Blog</a></li>
        <li class="nav-item"><a class="nav-link" href="https://dplatz.de/aiblog/">AI Blog</a></li>

        <li class="nav-item"><a class="nav-link" href="../../about.html">About</a></li>
        <li class="nav-item"><a class="nav-link" href="../../archive.html">Archive</a></li>
        <li class="nav-item"><a class="nav-link" href="../../tags/">Tags</a></li>
        <li class="nav-item"><a class="nav-link" href="../../feed.xml">Subscribe</a></li>
        <!--li  class="nav-item dropdown">
          <a href="#" class="dropdown-toggle" data-toggle="dropdown">Notes <b class="caret"></b></a>
          <ul class="dropdown-menu">
            <li><a href="git.html">Git</a></li>
            <li><a href="wildfly.html">Wildfly</a></li>
            
          </ul>
        </li-->

        <li class="nav-item">
          <a class="nav-link" href="https://github.com/38leinaD"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
          </span><span class="username">38leinaD</span></a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="https://twitter.com/38leinaD"><span class="icon icon--twitter"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
          </span><span class="username">38leinaD</span></a>
        </li>
      </ul>
    </div>
  </div>
</nav>

<div class="container">
	
	<div class="page-header">
		<h1>Running a local LLM on Ollama and LangChain4J</h1>
	</div>

	<p class="postingdate"><em>01 January 2024</em></p>

	<p><div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>When working on my personal projects, I see myself quiet often reaching out to ChatGPT for a quick explaination or tip. Unfortunately, this is not possible/allowed in most corporate environments.
That&#8217;s why I was naturally very interested in what it takes to run a Large Language Model (LLM) locally on my machine and maybe even enrich it with some domain knowledge and see if it can help in my daytime job for some use-case (either by fine-tuning wich is harder or by retrieval augmented generation (RAG)).</p>
</div>
<div class="paragraph">
<p>The AI journey for me just started, that&#8217;s why the goal of this post is to show how easy it is to run an LLM locally and access it from a Java-based application leveraging <a href="https://github.com/langchain4j/langchain4j">LangChain4J</a>.
As everyone has some favorite tools in his belt, it is natural to use them. That&#8217;s why my code example below is a self-contained JBang script that is leverarging <a href="https://quarkus.io/">Quarkus</a> and it&#8217;s <a href="https://github.com/quarkiverse/quarkus-langchain4j">LangChain4J extension</a>.
You can just as easily cut Quarkus out of the picture and use LangChain4J directly, but I was especially interested in the state of the Quarkus Integration for LangChain4J.</p>
</div>
<div class="paragraph">
<p>So, as you might have understood by now, LangChain4J is a big part of what allows you to access an LLM. What is important to understand here, is that it is only an abstraction to program against different AI services. LangChain4J does not actually run/host an LLM.
For that we will need another service that runs the LLM and exposes it so LangChain4J can access it. As a matter of fact, LangChain4J can integrate with OpenAI&#8217;s GPT models as they expose a Restful API for it.
In a similar fashion we can run an LLM locally with <a href="https://ollama.ai/">Ollama</a> and configure the exposed Restful endpoint for LangChain4J to use.
As this is just the beginning of the journey for me, I cannot explain to you what it would take to run/host an LLM in Java natively. For sure it must be technically possible, but then again, what is the big benefit?</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_installing_ollama">Installing Ollama</h2>
<div class="sectionbody">
<div class="paragraph">
<p>So, first step is to install Ollama. I ran it under WSL on a Windows machine and the steps you can find <a href="https://ollama.ai/download/linux">here</a> are as simple as they get:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>curl https://ollama.ai/install.sh | sh</pre>
</div>
</div>
<div class="paragraph">
<p>After this you need to download a model and then can interact with it via the commandline. If you have an already rather old Graphics cards like an NVidia RTX 2060 (with 6 GB VRAM), you can run a mid-sized model like Mistral 7b without problems on your GPU alone.
Run <code>ollama run mistral</code> which will download the model and then start a prompt to interact with it. The download is 4 GB, so it might take a few minutes depending on your internet speed.
If you feel like your PC is not capable of running this model, maybe try <a href="https://ollama.ai/library/orca-mini/tags">orca-mini</a> instead and run <code>ollama run orca-mini:3b</code>.
Generally, the models should be capable to run on a compatible GPU or fall back to running on the CPU. In case of running on the CPU, you will need a corresponding amount of RAM to load it.</p>
</div>
<div class="paragraph">
<p>Ollama will install as a service and expose a Restful API on port 11434. So, instead of using the command prompt you can also try to hit it via curl for a first test:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>curl -i -X POST http://127.0.0.1:11434/api/generate -d '{"model": "mistral", "prompt": "Why is the sky blue?"}'</pre>
</div>
</div>
<div class="paragraph">
<p>Note that you have to provide your model that you download before as the <code>model</code> parameter.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_quarkus_and_langchain4j">Quarkus and Langchain4J</h2>
<div class="sectionbody">
<div class="paragraph">
<p>If this is working we can come to the next step and use the LLM from within our Java application. For that, we need the LangChain4J library which can talk to our Ollama service.
Also, as I am a big fan of JBang and Quarkus, these were my natural choice for integrating with LangChain4J. But you can just as well use Langchain4J directly without any framework. See <a href="https://github.com/langchain4j/langchain4j/blob/main/langchain4j-ollama/src/test/java/dev/langchain4j/model/ollama/OllamaLanguageModelIT.java">this test</a> for the most basic integration between Langchain4J and Ollama.</p>
</div>
<div class="paragraph">
<p>Now lets come to this self-contained JBang script that will interact with the Ollama-based LLM:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="prettyprint highlight"><code data-lang="java">///usr/bin/env jbang "$0" "$@" ; exit $?
//DEPS io.quarkus.platform:quarkus-bom:3.6.4@pom
//DEPS io.quarkus:quarkus-picocli
//DEPS io.quarkus:quarkus-arc
//DEPS io.quarkiverse.langchain4j:quarkus-langchain4j-ollama:0.5.1 <b class="conum">(1)</b>

//JAVAC_OPTIONS -parameters
//JAVA_OPTIONS -Djava.util.logging.manager=org.jboss.logmanager.LogManager

//Q:CONFIG quarkus.banner.enabled=false
//Q:CONFIG quarkus.log.level=WARN
//Q:CONFIG quarkus.log.category."dev.langchain4j".level=DEBUG
//Q:CONFIG quarkus.langchain4j.ollama.chat-model.model-id=mistral <b class="conum">(2)</b>

import static java.lang.System.out;

import com.fasterxml.jackson.annotation.JsonCreator;

import dev.langchain4j.service.SystemMessage;
import dev.langchain4j.service.UserMessage;
import io.quarkiverse.langchain4j.RegisterAiService;
import jakarta.enterprise.context.control.ActivateRequestContext;
import jakarta.inject.Inject;
import picocli.CommandLine;

@CommandLine.Command
public class QuarkusLangchainOllama implements Runnable {

    @Inject
    TriageService triage;

    @Override
    @ActivateRequestContext <b class="conum">(3)</b>
    public void run() {
        String review = "I really love this bank. Not!";
        out.println("Review: " + review);
        out.println("...");
        TriagedReview result = triage.triage(review);

        out.println("Sentiment: " + result.evaluation());
        out.println("Message: " + result.message());
    }
}

@RegisterAiService
interface TriageService {
    @SystemMessage("""
        You are working for a bank, processing reviews about
        financial products. Triage reviews into positive and
        negative ones, responding with a JSON document.
        """
    )
    @UserMessage("""
        Your task is to process the review delimited by ---.
        Apply sentiment analysis to the review to determine
        if it is positive or negative, considering various languages.

        For example:
        - `I love your bank, you are the best!` is a 'POSITIVE' review
        - `J'adore votre banque` is a 'POSITIVE' review
        - `I hate your bank, you are the worst!` is a 'NEGATIVE' review

        Respond with a JSON document containing:
        - the 'evaluation' key set to 'POSITIVE' if the review is
        positive, 'NEGATIVE' otherwise
        - the 'message' key set to a message thanking or apologizing
        to the customer. These messages must be polite and match the
        review's language.

        ---
        {review}
        ---
    """)
    TriagedReview triage(String review);
}

record TriagedReview(Evaluation evaluation, String message) {
    @JsonCreator
    public TriagedReview {}
}

enum Evaluation {
    POSITIVE,
    NEGATIVE
}</code></pre>
</div>
</div>
<div class="colist arabic">
<ol>
<li>
<p>The required dependency to interact with Ollama.</p>
</li>
<li>
<p>The model needs to be configured as this is needed for the <code>model</code> parameter in the Restful request to Ollama.</p>
</li>
<li>
<p>Without this, I got an error that RequestScope is not initalized. But the error-message from Quarkus was very helpful and directly gave me the solution.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>You can find the source-code/the JBang script <a href="https://github.com/38leinaD/jbang-catalog/blob/master/src/QuarkusLangchainOllama.java">here</a>.
I don&#8217;t want to explain the main code that much as I just took the example from this awesome <a href="https://quarkus.io/blog/quarkus-meets-langchain4j/">LangChain4J post by the Quarkus guys</a> and you can read about it over there, but I think there is one quiet awesome fact that needs to be pointed out about it:
In the prompt we are telling the LLM to return a JSON structure with specific key names. Based on this, we are setting up our JSON-serializable POJOs named <code>TriageReview</code> and <code>Evaluation</code>.
In case the LLM returns a correct JSON structure (which the Mistral model did for me), Quarkus can deserialize it into an instance of <code>TriagedReview</code>. So, even though LLMs are widely seen as chat bots and usally return human-readable text, it is not limited to this.
There is no need to do any kind of manual parsing of the responses. As it is directly returning JSON, it is just as if you were calling an Restful endpoint via an OpenAI specification.</p>
</div>
<div class="paragraph">
<p>As I was saying before, LangChain4J offers an abstraction over different AI services. You could have skipped the setup of Ollama completly and just tried it out with OpenAI&#8217;s GPT-3 or GPT-4. The main difference would have just been to change the dependency from <code>io.quarkiverse.langchain4j:quarkus-langchain4j-ollama:0.5.1</code> to <code>io.quarkiverse.langchain4j:quarkus-langchain4j-openai:0.5.1</code>.</p>
</div>
<div class="paragraph">
<p>The last thing to do is to run the script via the JBang CLI. It should rate the sentiment of the given comment as negative in case it works as expected.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>jbang run --quiet QuarkusLangchainOllama.java</pre>
</div>
</div>
<div class="paragraph">
<p>Have fun with it.</p>
</div>
</div>
</div></p>

	<hr />

	<script src="https://giscus.app/client.js"
        data-repo="38leinaD/38leinaD.github.io"
        data-repo-id="MDEwOlJlcG9zaXRvcnk1NjkyNDIzOA=="
        data-category="Q&A"
        data-category-id="DIC_kwDOA2SYTs4CcH2P"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="light"
        data-lang="en"
        crossorigin="anonymous"
        async>
	</script>
	<br/>
		</div>
		<div id="push"></div>
    </div>
    
    <div id="footer">
      <div class="container">
        <p class="muted credit">&copy; 2018 | Mixed with <a href="http://getbootstrap.com/">Bootstrap v3.1.1</a> | Baked with <a href="http://jbake.org">JBake v2.7.0-rc.7</a></p>
      </div>
    </div>
    
    <!-- Le javascript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="../../js/jquery.min.js"></script>
    <script src="../../js/popper.min.js"></script>
    <script src="../../js/bootstrap.min.js"></script>
    <script src="../../js/prettify.js"></script>
    <script src="../../js/custom.js"></script>
  </body>
</html>
